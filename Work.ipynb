{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\leigh\\anaconda3\\lib\\site-packages (0.29.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gymnasium) (4.8.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gymnasium) (1.25.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.6.0)\n",
      "Requirement already satisfied: pyglet in c:\\users\\leigh\\anaconda3\\lib\\site-packages (2.0.9)\n",
      "Requirement already satisfied: pygame in c:\\users\\leigh\\anaconda3\\lib\\site-packages (2.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install pyglet\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leigh\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Score: 11.0 | 11 steps | 1.30s.\n",
      "Episode 1 | Score: 10.0 | 10 steps | 1.11s.\n",
      "Episode 2 | Score: 9.0 | 9 steps | 1.01s.\n",
      "Episode 3 | Score: 11.0 | 11 steps | 1.21s.\n",
      "Episode 4 | Score: 22.0 | 22 steps | 2.31s.\n",
      "Episode 5 | Score: 19.0 | 19 steps | 2.01s.\n",
      "Episode 6 | Score: 17.0 | 17 steps | 1.81s.\n",
      "Episode 7 | Score: 11.0 | 11 steps | 1.21s.\n",
      "Episode 8 | Score: 9.0 | 9 steps | 1.01s.\n",
      "Episode 9 | Score: 12.0 | 12 steps | 1.31s.\n",
      "Episode 10 | Score: 40.0 | 40 steps | 4.12s.\n",
      "Episode 11 | Score: 23.0 | 23 steps | 2.42s.\n",
      "Episode 12 | Score: 37.0 | 37 steps | 3.82s.\n",
      "Episode 13 | Score: 33.0 | 33 steps | 3.41s.\n",
      "Episode 14 | Score: 50.0 | 50 steps | 5.13s.\n",
      "Episode 15 | Score: 26.0 | 26 steps | 2.72s.\n"
     ]
    }
   ],
   "source": [
    "#This is the mannual game play cell\n",
    "import gymnasium as gym\n",
    "from gym.envs.classic_control.cartpole import *\n",
    "from pyglet.window import key\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gymnasium.utils.play import play\n",
    "\n",
    "bool_do_not_quit = True  # Boolean to quit pyglet\n",
    "scores = []  # Your gaming score\n",
    "a = 0  # Action\n",
    "total_reward = 0\n",
    "episode = 1\n",
    "steps = 0\n",
    "start_time = time.time()\n",
    "\n",
    "def cartpole_callback(obs, q_obs, action, reward, terminated, truncated, info):\n",
    "    global total_reward, steps, start_time\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% (time.time()-start_time))\n",
    "        scores.append(total_reward)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_cartPole_asHuman(policy=None):\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "    play(env, keys_to_action={\"a\":0, \"d\":1}, callback=cartpole_callback, fps=10)\n",
    "\n",
    "\n",
    "\n",
    "run_cartPole_asHuman() \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "print(\"Average Score for \", len(scores), \" episodes is:\",statistics.mean(scores))\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My human performance on CartPole-v1')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Human Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.classic_control.cartpole import *\n",
    "from pyglet.window import key\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "bool_do_not_quit = True  # Boolean to quit pyglet\n",
    "scores = []  # Your gaming score\n",
    "a = 0  # Action\n",
    "env = CartPoleEnv()\n",
    "\n",
    "number_of_trials = 5000\n",
    "\n",
    "def key_press(k, mod):\n",
    "    global bool_do_not_quit, a, restart\n",
    "    if k==0xff0d: restart = True\n",
    "    if k==key.ESCAPE: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.Q: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.LEFT:  a = 0  # 0     Push cart to the left\n",
    "    if k==key.RIGHT: a = 1  # 1     Push cart to the right\n",
    "\n",
    "def dummy_policy(state=0):\n",
    "    a = env.action_space.sample()\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime,os\n",
    "import collections\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(32,activation='tanh')(inputs)\n",
    "    x = layers.Dense(64,activation='tanh')(x)\n",
    "\n",
    "    outputs = layers.Dense(2, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"dqn_model\")\n",
    "    #lr stands for learning rate\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer =optimizer,loss='mse',\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model= build_model()\n",
    "target_network = build_model()\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0/number_of_trials\n",
    "\n",
    "episode_reward_list = []     \n",
    "exploring_rate=0.0\n",
    "target_score = 200\n",
    "\n",
    "memory = collections.deque(maxlen=100000)\n",
    "batch_size=32\n",
    "max_reward = -10000\n",
    "target_update_steps = 20\n",
    "\n",
    "\n",
    "steps_to_target = 20\n",
    "done=False\n",
    "\n",
    "last_state = []\n",
    "a = []\n",
    "reward = 0\n",
    "total_reward = 0.0\n",
    "\n",
    "\n",
    "\n",
    "#Get the predictd best action by our network\n",
    "def deep_q_policy(state):\n",
    "            \n",
    "    # Get the model's reward estimations of each Q(s,a)\n",
    "    action_q_values = model.predict([[state[0],state[1],state[2],state[3]]])\n",
    "    \n",
    "    # If exploiting, choose the model's estimated best action \n",
    "    return np.argmax(action_q_values)\n",
    "\n",
    "\n",
    "\n",
    "#Change our exploring rate so that later rounds are less random later on (because our network will hopefully be improving)\n",
    "def update_exploring_rate(roundNumber):\n",
    "    global exploring_rate\n",
    "    exploring_rate = 1-roundNumber*epsilon \n",
    "    if exploring_rate<0:\n",
    "        exploring_rate = 0.01\n",
    "\n",
    "\n",
    "        \n",
    "# Feed our network the batch of state changes and rewards\n",
    "def experience_replay():\n",
    "    global memory\n",
    "    state_batch, q_batch = [], []\n",
    "    # Get a batch of random state changes from memory to train our network\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
    "    for m_state, m_action, m_reward, m_next_state, m_done in minibatch:\n",
    "        if len(m_state) > 0:\n",
    "            m_q_values = model.predict([[m_state[0],m_state[1],m_state[2],m_state[3]]])\n",
    "            \n",
    "            #How good is this action?\n",
    "            if m_done :\n",
    "                m_q_values[0][m_action] = m_reward\n",
    "            else:  \n",
    "                m_q_values[0][m_action] = m_reward + gamma * np.max(target_network.predict([[m_next_state[0],m_next_state[1], m_next_state[2], m_next_state[3]]])[0])\n",
    "\n",
    "                \n",
    "            #Add to the list of states\n",
    "            state_batch.append([m_state[0],m_state[1],m_state[2],m_state[3]])\n",
    "            \n",
    "            #Add to the list of q values\n",
    "            q_batch.append(m_q_values[0])\n",
    "    \n",
    "    # Train on the batch\n",
    "    loss = model.train_on_batch(np.array(state_batch),np.array(q_batch))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "#Evaluate our model and, if it is better than the previous best, save the new weights\n",
    "def update_scores():\n",
    "    global max_reward\n",
    "    round_reward = scores[len(scores)-1]\n",
    "    if round_reward >= max_reward:\n",
    "        model.save(\"./dqn_exp_replay_tgtnetwork.h5\")\n",
    "        max_reward = round_reward\n",
    "    scores.append(round_reward)\n",
    "    episode_reward_list.append(round_reward)\n",
    "\n",
    "\n",
    "    \n",
    "#Update the target network\n",
    "def update_target():\n",
    "    global steps_to_target\n",
    "    # Update target network if it is time \n",
    "    if steps_to_target<=0:\n",
    "        target_network.set_weights(model.get_weights())\n",
    "        steps_to_target = target_update_steps\n",
    "    # Otherwise count down\n",
    "    else:\n",
    "        steps_to_target-=1\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#Deep q-learning algorithm using experience replay and a target network\n",
    "def dql_with_experience_replay_target(state):\n",
    "    global exploring_rate\n",
    "    global epsilon\n",
    "    global memory\n",
    "    global done\n",
    "    global reward\n",
    "    \n",
    "    # Update the exploring rate for the first round and \n",
    "    if done or len(scores) <= 0:\n",
    "        update_exploring_rate(len(scores)+1)\n",
    "        print(exploring_rate*100, \"% random\")\n",
    "    \n",
    "    # Update scores and target after each round\n",
    "    if done:\n",
    "        update_target()\n",
    "        update_scores()\n",
    "        loss = experience_replay()\n",
    "    \n",
    "    # Add each state change and its reward to memory for experience replay\n",
    "    if len(last_state) >= 0:\n",
    "        memory.append((last_state, a, reward, state, done))\n",
    "    \n",
    "    # Decide whether we should explore or exploit\n",
    "    exploring =  np.random.choice([True,False], p=[exploring_rate,1-exploring_rate])\n",
    "            \n",
    "    \n",
    "    if exploring: # Exploration\n",
    "        action = env.action_space.sample() # Random action\n",
    "    else: # Exploitation\n",
    "        action = deep_q_policy(state) # Estimated best action\n",
    "        \n",
    "    \n",
    "    return action\n",
    "    \n",
    "\n",
    "\n",
    "def run_cartPole_asAgent(policy=None):\n",
    "    global last_state\n",
    "    global a\n",
    "    global reward\n",
    "    global done\n",
    "    \n",
    "    env.reset()\n",
    "    env.render()\n",
    "    env.viewer.window.on_key_press = key_press\n",
    "\n",
    "    for _ in range(number_of_trials):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        restart = False\n",
    "        t1 = time.time()  # Trial timer\n",
    "        while bool_do_not_quit:\n",
    "            #this is where policy function outputs action a based on the current state\n",
    "            a = policy(state)\n",
    "            last_state = copy.copy(state)\n",
    "            #this is there you get the next system state after take action a\n",
    "            state, reward, done, info = env.step(a)\n",
    "            time.sleep(1/10)  # 10fps: Super slow for us poor little human!\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            env.render()\n",
    "            if done or restart:\n",
    "                t1 = time.time()-t1\n",
    "                scores.append(total_reward)\n",
    "                print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% t1)\n",
    "                break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run_cartPole_asAgent(policy=dql_with_experience_replay_target)  # Run with agent input\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "print(\"Average Score for \", number_of_trials, \" episodes is:\",statistics.mean(scores))\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My agent performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Agent Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the best agent off playing\n",
    "\n",
    "def play_best():\n",
    "    model.load_weights(\"./best_dqn.h5\")\n",
    "    run_cartPole_asAgent(policy=deep_q_policy)\n",
    "\n",
    "play_best()\n",
    "\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My agent performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Agent Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
