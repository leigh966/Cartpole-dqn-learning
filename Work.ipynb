{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\leigh\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gym) (1.20.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from gym) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leigh\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.6.0)\n",
      "Requirement already satisfied: pyglet in c:\\users\\leigh\\anaconda3\\lib\\site-packages (2.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install gym[classic_control]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgfxdraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26924/194034259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mrun_cartPole_asHuman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26924/194034259.py\u001b[0m in \u001b[0;36mrun_cartPole_asHuman\u001b[1;34m(policy)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_key_press\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_press\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgfxdraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             raise DependencyNotInstalled(\n\u001b[0m\u001b[0;32m    223\u001b[0m                 \u001b[1;34m\"pygame is not installed, run `pip install gym[classic_control]`\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             )\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[classic_control]`"
     ]
    }
   ],
   "source": [
    "#This is the mannual game play cell\n",
    "import gym\n",
    "from gym.envs.classic_control.cartpole import *\n",
    "from pyglet.window import key\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "bool_do_not_quit = True  # Boolean to quit pyglet\n",
    "scores = []  # Your gaming score\n",
    "a = 0  # Action\n",
    "\n",
    "def key_press(k, mod):\n",
    "    global bool_do_not_quit, a, restart\n",
    "    if k==0xff0d: restart = True\n",
    "    if k==key.ESCAPE: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.Q: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.LEFT:  a = 0  # 0     Push cart to the left\n",
    "    if k==key.RIGHT: a = 1  # 1     Push cart to the right\n",
    "\n",
    "def run_cartPole_asHuman(policy=None):\n",
    "    env = CartPoleEnv()\n",
    "\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    env.viewer.window.on_key_press = key_press\n",
    "\n",
    "    while bool_do_not_quit:\n",
    "        env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        restart = False\n",
    "        t1 = time.time()  # Trial timer\n",
    "        while bool_do_not_quit:\n",
    "            s, r, done, info = env.step(a)\n",
    "            time.sleep(1/10)  # 10fps: Super slow for us poor little human!\n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "        \n",
    "            env.render()\n",
    "            if done or restart:\n",
    "                t1 = time.time()-t1\n",
    "                scores.append(total_reward)\n",
    "                print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% t1)\n",
    "                break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run_cartPole_asHuman() \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "print(\"Average Score for \", len(scores), \" episodes is:\",statistics.mean(scores))\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My human performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Human Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leigh\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26924/2392588722.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m \u001b[0mrun_cartPole_asAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdql_with_experience_replay_target\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Run with agent input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26924/2392588722.py\u001b[0m in \u001b[0;36mrun_cartPole_asAgent\u001b[1;34m(policy)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_key_press\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_press\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;34m\"You are calling render method without specifying any render mode. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;34m\"You can specify the render_mode at initialization, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m                 \u001b[1;34mf'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             )\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.envs.classic_control.cartpole import *\n",
    "from pyglet.window import key\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "bool_do_not_quit = True  # Boolean to quit pyglet\n",
    "scores = []  # Your gaming score\n",
    "a = 0  # Action\n",
    "env = CartPoleEnv()\n",
    "\n",
    "number_of_trials = 5000\n",
    "\n",
    "def key_press(k, mod):\n",
    "    global bool_do_not_quit, a, restart\n",
    "    if k==0xff0d: restart = True\n",
    "    if k==key.ESCAPE: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.Q: bool_do_not_quit=False  # Added to Quit\n",
    "    if k==key.LEFT:  a = 0  # 0     Push cart to the left\n",
    "    if k==key.RIGHT: a = 1  # 1     Push cart to the right\n",
    "\n",
    "def dummy_policy(state=0):\n",
    "    a = env.action_space.sample()\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime,os\n",
    "import collections\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(32,activation='tanh')(inputs)\n",
    "    x = layers.Dense(64,activation='tanh')(x)\n",
    "\n",
    "    outputs = layers.Dense(2, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"dqn_model\")\n",
    "    #lr stands for learning rate\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer =optimizer,loss='mse',\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model= build_model()\n",
    "target_network = build_model()\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0/number_of_trials\n",
    "\n",
    "episode_reward_list = []     \n",
    "exploring_rate=0.0\n",
    "target_score = 200\n",
    "\n",
    "memory = collections.deque(maxlen=100000)\n",
    "batch_size=32\n",
    "max_reward = -10000\n",
    "target_update_steps = 20\n",
    "\n",
    "\n",
    "steps_to_target = 20\n",
    "done=False\n",
    "\n",
    "last_state = []\n",
    "a = []\n",
    "reward = 0\n",
    "total_reward = 0.0\n",
    "\n",
    "\n",
    "\n",
    "#Get the predictd best action by our network\n",
    "def deep_q_policy(state):\n",
    "            \n",
    "    # Get the model's reward estimations of each Q(s,a)\n",
    "    action_q_values = model.predict([[state[0],state[1],state[2],state[3]]])\n",
    "    \n",
    "    # If exploiting, choose the model's estimated best action \n",
    "    return np.argmax(action_q_values)\n",
    "\n",
    "\n",
    "\n",
    "#Change our exploring rate so that later rounds are less random later on (because our network will hopefully be improving)\n",
    "def update_exploring_rate(roundNumber):\n",
    "    global exploring_rate\n",
    "    exploring_rate = 1-roundNumber*epsilon \n",
    "    if exploring_rate<0:\n",
    "        exploring_rate = 0.01\n",
    "\n",
    "\n",
    "        \n",
    "# Feed our network the batch of state changes and rewards\n",
    "def experience_replay():\n",
    "    global memory\n",
    "    state_batch, q_batch = [], []\n",
    "    # Get a batch of random state changes from memory to train our network\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
    "    for m_state, m_action, m_reward, m_next_state, m_done in minibatch:\n",
    "        if len(m_state) > 0:\n",
    "            m_q_values = model.predict([[m_state[0],m_state[1],m_state[2],m_state[3]]])\n",
    "            \n",
    "            #How good is this action?\n",
    "            if m_done :\n",
    "                m_q_values[0][m_action] = m_reward\n",
    "            else:  \n",
    "                m_q_values[0][m_action] = m_reward + gamma * np.max(target_network.predict([[m_next_state[0],m_next_state[1], m_next_state[2], m_next_state[3]]])[0])\n",
    "\n",
    "                \n",
    "            #Add to the list of states\n",
    "            state_batch.append([m_state[0],m_state[1],m_state[2],m_state[3]])\n",
    "            \n",
    "            #Add to the list of q values\n",
    "            q_batch.append(m_q_values[0])\n",
    "    \n",
    "    # Train on the batch\n",
    "    loss = model.train_on_batch(np.array(state_batch),np.array(q_batch))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "#Evaluate our model and, if it is better than the previous best, save the new weights\n",
    "def update_scores():\n",
    "    global max_reward\n",
    "    round_reward = scores[len(scores)-1]\n",
    "    if round_reward >= max_reward:\n",
    "        model.save(\"./dqn_exp_replay_tgtnetwork.h5\")\n",
    "        max_reward = round_reward\n",
    "    scores.append(round_reward)\n",
    "    episode_reward_list.append(round_reward)\n",
    "\n",
    "\n",
    "    \n",
    "#Update the target network\n",
    "def update_target():\n",
    "    global steps_to_target\n",
    "    # Update target network if it is time \n",
    "    if steps_to_target<=0:\n",
    "        target_network.set_weights(model.get_weights())\n",
    "        steps_to_target = target_update_steps\n",
    "    # Otherwise count down\n",
    "    else:\n",
    "        steps_to_target-=1\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#Deep q-learning algorithm using experience replay and a target network\n",
    "def dql_with_experience_replay_target(state):\n",
    "    global exploring_rate\n",
    "    global epsilon\n",
    "    global memory\n",
    "    global done\n",
    "    global reward\n",
    "    \n",
    "    # Update the exploring rate for the first round and \n",
    "    if done or len(scores) <= 0:\n",
    "        update_exploring_rate(len(scores)+1)\n",
    "        print(exploring_rate*100, \"% random\")\n",
    "    \n",
    "    # Update scores and target after each round\n",
    "    if done:\n",
    "        update_target()\n",
    "        update_scores()\n",
    "        loss = experience_replay()\n",
    "    \n",
    "    # Add each state change and its reward to memory for experience replay\n",
    "    if len(last_state) >= 0:\n",
    "        memory.append((last_state, a, reward, state, done))\n",
    "    \n",
    "    # Decide whether we should explore or exploit\n",
    "    exploring =  np.random.choice([True,False], p=[exploring_rate,1-exploring_rate])\n",
    "            \n",
    "    \n",
    "    if exploring: # Exploration\n",
    "        action = env.action_space.sample() # Random action\n",
    "    else: # Exploitation\n",
    "        action = deep_q_policy(state) # Estimated best action\n",
    "        \n",
    "    \n",
    "    return action\n",
    "    \n",
    "\n",
    "\n",
    "def run_cartPole_asAgent(policy=None):\n",
    "    global last_state\n",
    "    global a\n",
    "    global reward\n",
    "    global done\n",
    "    \n",
    "    env.reset()\n",
    "    env.render()\n",
    "    env.viewer.window.on_key_press = key_press\n",
    "\n",
    "    for _ in range(number_of_trials):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        restart = False\n",
    "        t1 = time.time()  # Trial timer\n",
    "        while bool_do_not_quit:\n",
    "            #this is where policy function outputs action a based on the current state\n",
    "            a = policy(state)\n",
    "            last_state = copy.copy(state)\n",
    "            #this is there you get the next system state after take action a\n",
    "            state, reward, done, info = env.step(a)\n",
    "            time.sleep(1/10)  # 10fps: Super slow for us poor little human!\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            env.render()\n",
    "            if done or restart:\n",
    "                t1 = time.time()-t1\n",
    "                scores.append(total_reward)\n",
    "                print(\"Episode\", len(scores), \"| Score:\", total_reward, '|', steps, \"steps | %0.2fs.\"% t1)\n",
    "                break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run_cartPole_asAgent(policy=dql_with_experience_replay_target)  # Run with agent input\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "print(\"Average Score for \", number_of_trials, \" episodes is:\",statistics.mean(scores))\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My agent performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Agent Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the best agent off playing\n",
    "\n",
    "def play_best():\n",
    "    model.load_weights(\"./best_dqn.h5\")\n",
    "    run_cartPole_asAgent(policy=deep_q_policy)\n",
    "\n",
    "play_best()\n",
    "\n",
    "# Plot your score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My agent performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Agent Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
